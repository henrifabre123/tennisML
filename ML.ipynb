{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette première partie dédiée au machine learning, nous allons procédé à une prédiction du classement des joueurs du top 100 en fonction des statistiques de leur saison. Dans un premier temps, nous allons utiliser un modèle 'random forest', puis nous allons le comparer avec un autre modèle le 'GradientBoostingRegressor'. Enfin, nous allons modifier les paramètres de configuration du modèle précédent pour en trouver un assez performant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chargement des données\n",
    "base_path = './Data/Data_utiles/Data_ML/'\n",
    "\n",
    "# Liste pour stocker chaque DataFrame\n",
    "dataframes = []\n",
    "\n",
    "# Boucle pour lire chaque fichier CSV de 1995 à 2018\n",
    "for year in range(1995, 2022):\n",
    "    file_path = f'{base_path}infos_joueurs_{year}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "columns = [\n",
    "    \"name\", \"hand\", \"height\", \"rang\", \"matchs\", \"win\", \n",
    "    \"pourc_return_win_pnt\", \"pourc_break_games\", \"pourc_break_point_made\", \n",
    "    \"pourc_break_point_saved\", \"pourc_serv_games_win\", \"pourc_serv_in\", \n",
    "    \"mean_ranking_oppo\", \"pourc_serv_win_pnt\", \"Return Rating\", \n",
    "    \" % Serve Return Points Won\", \" % 2nd Serve Return Points Won\", \n",
    "    \" % Return Games Won\", \" % Break Points Converted\", \"Under Pressure Rating\", \n",
    "    \" % Break Point Saved\", \" % Break Points Converted Pressure\", \n",
    "    \" % Deciding Sets Won\", \" % Tie Breaks Won\"\n",
    "]\n",
    " # Complétez avec les autres noms de colonnes\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df['hand'] = df['hand'].map({'R': 1, 'L': 0})\n",
    "\n",
    "# Vérifier les modifications\n",
    "print(df['hand'].head())\n",
    "print(df.columns)\n",
    "\n",
    "# Séparation des caractéristiques et des étiquettes\n",
    "features = df.drop(['name','rang','Under Pressure Rating', ' % Break Point Saved', \n",
    "    ' % Break Points Converted Pressure', ' % Deciding Sets Won', \n",
    "    ' % Tie Breaks Won'], axis=1)\n",
    "labels = df['rang']\n",
    "  # Affiche les premières lignes pour vérifier les données\n",
    "indices_with_nan = features[features.isna().any(axis=1)].index\n",
    "\n",
    "# Supprimer ces lignes de features\n",
    "features_clean = features.dropna()\n",
    "\n",
    "# Supprimer les mêmes lignes de labels\n",
    "labels_clean = labels.drop(indices_with_nan) # Affiche la taille de l'ensemble des étiquettes\n",
    "print(features_clean.shape)  # Doit montrer (N, nombre_de_colonnes)\n",
    "print(labels_clean.shape) \n",
    "# Prétraitement : Encodage one-hot pour les variables catégorielles et normalisation pour les numériques\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['hand']),\n",
    "        ('num', StandardScaler(), features.columns.drop('hand'))\n",
    "    ])\n",
    "\n",
    "# Pipeline : Prétraitement + modèle\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_clean, labels_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vérification et suppression des NaN dans les ensembles d'entraînement et de test\n",
    "X_train = X_train.dropna()\n",
    "y_train = y_train[X_train.index]  # Assurez-vous que les indices correspondent\n",
    "X_test = X_test.dropna()\n",
    "y_test = y_test[X_test.index]\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation du modèle\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "#réalisation de cross validation\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Utilisation de KFold pour un contrôle plus précis\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Évaluation du modèle en utilisant la cross-validation avec le pipeline\n",
    "scores = cross_val_score(model, features_clean, labels_clean, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "print(\"RMSE par fold : \", rmse_scores)\n",
    "print(\"RMSE moyen : \", rmse_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième modèle de ML :GradientBoostingRegressor\n",
    "GradientBoostingRegressor est un modèle puissant qui construit des arbres de décision de manière séquentielle, où chaque arbre tente de corriger les erreurs des arbres précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE sur l'ensemble de test : 13.83982480044501\n",
      "RMSE par fold : [13.8398248  13.25627024 13.39855893 14.40236881 14.58997891]\n",
      "RMSE moyen : 13.89740033870961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# Prétraitement : Encodage one-hot pour les variables catégorielles et normalisation pour les numériques\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['hand']),  # Assurez-vous que 'hand' est la colonne catégorielle\n",
    "        ('num', StandardScaler(), features_clean.select_dtypes(include=['float64', 'int64']).columns)\n",
    "    ])\n",
    "\n",
    "# Création du pipeline avec GradientBoostingRegressor\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_clean, labels_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"RMSE sur l'ensemble de test :\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# Réalisation de la cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, features_clean, labels_clean, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "print(\"RMSE par fold :\", rmse_scores)\n",
    "print(\"RMSE moyen :\", rmse_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Meilleurs paramètres :  {'regressor__learning_rate': 0.01, 'regressor__max_depth': 3, 'regressor__n_estimators': 300}\n",
      "RMSE sur l'ensemble de test : 13.926325636250269\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Création du pipeline\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Paramètres pour la recherche en grille\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'regressor__max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Recherche en grille avec validation croisée\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(features_clean, labels_clean)\n",
    "\n",
    "# Meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Afficher les meilleurs paramètres\n",
    "print(\"Meilleurs paramètres : \", grid_search.best_params_)\n",
    "\n",
    "# Évaluation du modèle\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_clean, labels_clean, test_size=0.2, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE sur l'ensemble de test :\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19901231', '19911230', '19921228', '19931227', '19941226', '19951225', '19961230', '19971229', '19981228', '19991227', '20001225', '20011231', '20021230', '20031229', '20041227', '20051226', '20061225', '20071231', '20081229', '20091228', '20101227', '20111226', '20121231', '20131230', '20141229', '20151228', '20161226', '20171225', '20181231', '20191230', '20201228', '20211227', '20221226']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
